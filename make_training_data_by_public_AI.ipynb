{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Gemini API 기반 데이터 생성 스크립트\n",
    "##### ~~pip install -U google-generativeai~~\n",
    "##### pip install google-genai"
   ],
   "id": "6fbb82bd5cc56e75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T02:03:51.244257Z",
     "start_time": "2026-01-12T02:03:50.549828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "from config import config\n",
    "\n",
    "client = genai.Client(api_key=config.gemini_api_key)\n",
    "\n",
    "for m in client.models.list():\n",
    "    print(f\"Model Name: {m.name}\")\n",
    "    # print(f\"Model Name: {m.name}, Supported Methods: {m.model_fields}\")"
   ],
   "id": "7f47e703bd99e3bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: models/embedding-gecko-001\n",
      "Model Name: models/gemini-2.5-flash\n",
      "Model Name: models/gemini-2.5-pro\n",
      "Model Name: models/gemini-2.0-flash-exp\n",
      "Model Name: models/gemini-2.0-flash\n",
      "Model Name: models/gemini-2.0-flash-001\n",
      "Model Name: models/gemini-2.0-flash-exp-image-generation\n",
      "Model Name: models/gemini-2.0-flash-lite-001\n",
      "Model Name: models/gemini-2.0-flash-lite\n",
      "Model Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Model Name: models/gemini-2.0-flash-lite-preview\n",
      "Model Name: models/gemini-exp-1206\n",
      "Model Name: models/gemini-2.5-flash-preview-tts\n",
      "Model Name: models/gemini-2.5-pro-preview-tts\n",
      "Model Name: models/gemma-3-1b-it\n",
      "Model Name: models/gemma-3-4b-it\n",
      "Model Name: models/gemma-3-12b-it\n",
      "Model Name: models/gemma-3-27b-it\n",
      "Model Name: models/gemma-3n-e4b-it\n",
      "Model Name: models/gemma-3n-e2b-it\n",
      "Model Name: models/gemini-flash-latest\n",
      "Model Name: models/gemini-flash-lite-latest\n",
      "Model Name: models/gemini-pro-latest\n",
      "Model Name: models/gemini-2.5-flash-lite\n",
      "Model Name: models/gemini-2.5-flash-image-preview\n",
      "Model Name: models/gemini-2.5-flash-image\n",
      "Model Name: models/gemini-2.5-flash-preview-09-2025\n",
      "Model Name: models/gemini-2.5-flash-lite-preview-09-2025\n",
      "Model Name: models/gemini-3-pro-preview\n",
      "Model Name: models/gemini-3-flash-preview\n",
      "Model Name: models/gemini-3-pro-image-preview\n",
      "Model Name: models/nano-banana-pro-preview\n",
      "Model Name: models/gemini-robotics-er-1.5-preview\n",
      "Model Name: models/gemini-2.5-computer-use-preview-10-2025\n",
      "Model Name: models/deep-research-pro-preview-12-2025\n",
      "Model Name: models/embedding-001\n",
      "Model Name: models/text-embedding-004\n",
      "Model Name: models/gemini-embedding-exp-03-07\n",
      "Model Name: models/gemini-embedding-exp\n",
      "Model Name: models/gemini-embedding-001\n",
      "Model Name: models/aqa\n",
      "Model Name: models/imagen-4.0-generate-preview-06-06\n",
      "Model Name: models/imagen-4.0-ultra-generate-preview-06-06\n",
      "Model Name: models/imagen-4.0-generate-001\n",
      "Model Name: models/imagen-4.0-ultra-generate-001\n",
      "Model Name: models/imagen-4.0-fast-generate-001\n",
      "Model Name: models/veo-2.0-generate-001\n",
      "Model Name: models/veo-3.0-generate-001\n",
      "Model Name: models/veo-3.0-fast-generate-001\n",
      "Model Name: models/veo-3.1-generate-preview\n",
      "Model Name: models/veo-3.1-fast-generate-preview\n",
      "Model Name: models/gemini-2.5-flash-native-audio-latest\n",
      "Model Name: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "Model Name: models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel # 구조화된 출력을 위해 권장\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from config import config\n",
    "\n",
    "# 1. Gemini API 설정\n",
    "client = genai.Client(api_key=config.gemini_api_key)\n",
    "\n",
    "# 모델 ID 설정\n",
    "MODEL_ID = config.gemini_model_id\n",
    "\n",
    "# 2. 상세 카테고리 정의\n",
    "categories = [\n",
    "    \"한국사 주요 사건\", \"기초 물리 법칙\", \"생활 속 화학 원리\", \"태양계와 우주 탐사\",\n",
    "    \"IT 트렌드 및 컴퓨터 용어\", \"기초 경제 용어\", \"세계의 지리와 문화\",\n",
    "    \"클래식 음악과 예술가\", \"스포츠 규칙 및 역사\", \"기후 변화와 환경 상식\"\n",
    "]\n",
    "\n",
    "def generate_knowledge_data(category, count=10):\n",
    "    prompt = f\"\"\"\n",
    "    너는 인공지능 학습을 위한 고품질 한국어 데이터셋 생성 전문가야.\n",
    "    {category} 분야에 관한 '일반 상식 및 정보' 데이터셋 {count}개를 생성해줘.\n",
    "\n",
    "    [조건]\n",
    "    1. 형식은 반드시 JSON 리스트 형태여야 함.\n",
    "    2. 리스트 내 객체는 \"instruction\"과 \"output\" 키만 가질 것.\n",
    "    3. 질문(instruction)은 간결하고, 답변(output)은 정확한 사실을 바탕으로 1~2문장으로 작성할 것.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 새로운 SDK의 generate_content 방식\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt,\n",
    "            config={  # config 매개변수 통해 JSON 출력 강제\n",
    "                'response_mime_type': 'application/json',\n",
    "            }\n",
    "        )\n",
    "        return json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. 데이터 수집 및 저장\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "output_file = \"./output/gemini_synthetic_data.jsonl\"\n",
    "\n",
    "for i in range(50):  # 50번 반복하여 총 50 * count개 생성\n",
    "    target_cat = categories[i % len(categories)]\n",
    "    print(f\"[{i+1}/50] {target_cat} 생성 중...\")\n",
    "\n",
    "    results = generate_knowledge_data(target_cat, count=10)\n",
    "\n",
    "    if results:\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            # 리스트 형태인 경우와 단일 객체인 경우 모두 대응\n",
    "            if isinstance(results, list):\n",
    "                for item in results:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            elif isinstance(results, dict):\n",
    "                # 가끔 모델이 리스트가 아닌 'data': [] 형태의 딕셔너리로 줄 때를 대비\n",
    "                items = results.get('data', [results])\n",
    "                for item in items:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # 무료 티어의 분당 호출 제한(RPM)을 피하기 위해 4~5초간 대기\n",
    "    time.sleep(4)\n",
    "\n",
    "print(f\"LLM 학습 데이터 생성 완료: {output_file}\")"
   ],
   "id": "98bff528b8a99cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T01:48:01.071037Z",
     "start_time": "2026-01-12T01:47:51.572997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import google.generativeai as genai  # # Deprecated\n",
    "# import json\n",
    "# import time\n",
    "# import os\n",
    "#\n",
    "# from config import config\n",
    "#\n",
    "# # 1. Gemini API 설정\n",
    "# # https://aistudio.google.com/ 에서 무료 API 키를 발급 필요\n",
    "# genai.configure(api_key=config.gemini_api_key)\n",
    "# model = genai.GenerativeModel(config.gemini_model_id)\n",
    "#\n",
    "# # 2. 상세 카테고리 정의\n",
    "# categories = [\n",
    "#     \"한국사 주요 사건\", \"기초 물리 법칙\", \"생활 속 화학 원리\", \"태양계와 우주 탐사\",\n",
    "#     \"IT 트렌드 및 컴퓨터 용어\", \"기초 경제 용어\", \"세계의 지리와 문화\",\n",
    "#     \"클래식 음악과 예술가\", \"스포츠 규칙 및 역사\", \"기후 변화와 환경 상식\"\n",
    "# ]\n",
    "#\n",
    "# def generate_knowledge_data(category, count=10):\n",
    "#     prompt = f\"\"\"\n",
    "#     너는 인공지능 학습을 위한 고품질 한국어 데이터셋 생성 전문가야.\n",
    "#     {category} 분야에 관한 '일반 상식 및 정보' 데이터셋 {count}개를 생성해줘.\n",
    "#\n",
    "#     [조건]\n",
    "#     1. 형식은 반드시 JSON 리스트 형태여야 함.\n",
    "#     2. 리스트 내 객체는 \"instruction\"과 \"output\" 키만 가질 것.\n",
    "#     3. 질문(instruction)은 간결하고, 답변(output)은 정확한 사실을 바탕으로 1~2문장으로 작성할 것.\n",
    "#     4. 마크다운 기호 없이 순수 JSON 코드만 출력해.\n",
    "#\n",
    "#     예시: {{\"instruction\": \"조선왕조실록이란?\", \"output\": \"조선 시대 왕들의 재위 기간 기록을 날짜순으로 정리한 역사 기록물입니다.\"}}\n",
    "#     \"\"\"\n",
    "#\n",
    "#     try:\n",
    "#         response = model.generate_content(prompt)\n",
    "#         # 텍스트에서 JSON 부분만 추출 (가끔 붙는 ```json 제거)\n",
    "#         clean_text = response.text.replace('```json', '').replace('```', '').strip()\n",
    "#         return json.loads(clean_text)\n",
    "#     except Exception as e:\n",
    "#         print(f\"오류 발생: {e}\")\n",
    "#         return None\n",
    "#\n",
    "# # 3. 데이터 수집 및 저장\n",
    "# output_file = \"./output/gemini_synthetic_data.jsonl\"\n",
    "#\n",
    "# for i in range(50):  # 50번 반복하여 총 500개 생성 (무료 티어 분당 호출 제한 주의)\n",
    "#     target_cat = categories[i % len(categories)]\n",
    "#     print(f\"[{i+1}/50] {target_cat} 생성 중...\")\n",
    "#\n",
    "#     results = generate_knowledge_data(target_cat, count=10)\n",
    "#\n",
    "#     if results:\n",
    "#         with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "#             for item in results:\n",
    "#                 f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "#\n",
    "#     # 무료 티어의 분당 호출 제한(RPM)을 피하기 위해 4~5초간 대기\n",
    "#     time.sleep(4)\n",
    "#\n",
    "# print(f\"LLM 학습 데이터 생성 완료: {output_file}\")"
   ],
   "id": "241e7133846aa271",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_analysis/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/anaconda3/envs/data_analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/b2/vx12c8bs3bgfsxvbxwr9kddm0000gn/T/ipykernel_23969/1511828891.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] 한국사 주요 사건 생성 중...\n",
      "오류 발생: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[2/50] 기초 물리 법칙 생성 중...\n",
      "오류 발생: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 58\u001B[0m\n\u001B[1;32m     55\u001B[0m                 f\u001B[38;5;241m.\u001B[39mwrite(json\u001B[38;5;241m.\u001B[39mdumps(item, ensure_ascii\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# 무료 티어의 분당 호출 제한(RPM)을 피하기 위해 4~5초간 대기\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLLM 학습 데이터 생성 완료: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 수천 개 데이터 품질 검수 스크립트\n",
    "##### Gemini가 간혹 \"데이터를 생성해 드릴 수 없습니다\"와 같은 거절 메시지를 포함할 수 있음. 이를 필터링하는 검수 코드."
   ],
   "id": "dcf779ec0eda3bc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_and_verify(file_path, res_fl_path=\"./output/final_cleaned_data.jsonl\"):\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # 1. 중복 질문 제거\n",
    "    df = df.drop_duplicates(subset=['instruction'])\n",
    "\n",
    "    # 2. 너무 짧거나 거절 메시지가 포함된 답변 제거\n",
    "    bad_keywords = [\"죄송합니다\", \"제공할 수 없습니다\", \"인공지능 비서\"]\n",
    "    df = df[~df['output'].str.contains('|'.join(bad_keywords))]\n",
    "\n",
    "    # 3. 글자 수 필터링 (너무 짧은 답변 제외)\n",
    "    df = df[df['output'].str.len() > 5]\n",
    "\n",
    "    print(f\"최종 정제된 데이터 개수: {len(df)}\")\n",
    "    df.to_json(res_fl_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "clean_and_verify(output_file)"
   ],
   "id": "ba2b0a81558a6c15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 학습 데이터 확장 로드맵\n",
    "##### 수천 개를 넘어 수만 개로 가려면 카테고리를 더 세분화.\n",
    "\n",
    "##### 세부 카테고리화: \"한국사\" 대신 \"조선 전기 사회상\", \"구한말 외교 관계\" 식으로.\n",
    "\n",
    "##### 역방향 생성: \"A는 B다\"라는 문장을 주고 \"이 문장이 답이 될 수 있는 질문을 만들어줘\"라고 요청하여 데이터의 다양성을 확보.\n",
    "\n",
    "##### 수집된 데이터 양이 2,000개가 넘어가면, TinyLlama 학습 시 learning_rate를 조금 낮추어(예: 1e-4) 더 정교하게 튜닝."
   ],
   "id": "62a21a2c176ede02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e4ce4990f726e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23ce1d11f8e16815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "979ee9caf5e0ebcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1efb929bf489e7e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6528a4d43a835da3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 데이터 자동 생성 파이썬 스크립트\n",
    "##### pip install openai\n",
    "##### 주제 지정 -> API 호출 -> 결과 검증 -> JSONL 저장"
   ],
   "id": "9b42d05d60e85e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import random\n",
    "\n",
    "from config import config\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "client = openai.OpenAI(api_key=config.open_ai_api_key)\n",
    "\n",
    "# 1. 데이터 생성을 위한 세부 카테고리 정의 (수천 개 확장을 위한 기반)\n",
    "categories = [\n",
    "    \"한국 역사\", \"세계사\", \"기초 과학(물리, 화학, 생물)\", \"천문학 및 우주\",\n",
    "    \"IT 및 컴퓨터 공학\", \"경제 및 경영 상식\", \"법률 및 정치\", \"지리 및 국가\",\n",
    "    \"예술 및 클래식 음악\", \"스포츠 상식\", \"환경 및 기후 변화\"\n",
    "]\n",
    "\n",
    "def generate_data(category, count=10):\n",
    "    \"\"\"\n",
    "    GPT를 사용하여 특정 카테고리의 지시사항-답변 쌍을 생성.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    너는 인공지능 학습을 위한 고품질 한국어 데이터셋 생성 전문가야.\n",
    "    {category} 분야에 관한 '일반 상식 및 정보' 데이터셋 {count}개를 생성해줘.\n",
    "\n",
    "    [조건]\n",
    "    1. 형식은 반드시 JSON 리스트 형태여야 함.\n",
    "    2. 키값은 \"instruction\"과 \"output\"으로 구성할 것.\n",
    "    3. 질문(instruction)은 명확해야 하며, 답변(output)은 1~2문장의 정확한 사실로 작성할 것.\n",
    "    4. TinyLlama-1.1B 모델이 학습하기 적좋은 짧고 간결한 문체를 사용할 것.\n",
    "\n",
    "    예시: {{\"instruction\": \"지구에서 가장 높은 산은?\", \"output\": \"에베레스트 산입니다.\"}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # 비용 효율적인 mini 모델 권장\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={ \"type\": \"json_object\" } # JSON 출력을 보장\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 2. 실행 및 파일 저장\n",
    "output_file = \"synthetic_knowledge_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10): # 예시로 10번 반복 (한 번에 10개씩 총 100개)\n",
    "        category = random.choice(categories)\n",
    "        print(f\"[{i+1}] {category} 데이터 생성 중...\")\n",
    "\n",
    "        try:\n",
    "            raw_result = generate_data(category, count=10)\n",
    "            json_result = json.loads(raw_result)\n",
    "\n",
    "            # JSON 내의 리스트 추출 (GPT 응답 구조에 따라 조정 필요)\n",
    "            items = json_result.get(\"data\", json_result.get(\"items\", json_result))\n",
    "            if isinstance(items, dict): # 딕셔너리 형태일 경우 리스트로 변환\n",
    "                items = list(items.values())[0]\n",
    "\n",
    "            for item in items:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "\n",
    "print(f\"데이터 생성이 완료되었습니다. 파일명: {output_file}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 수천 개 데이터 구축 시 핵심 전략\n",
    "##### 시드(Seed) 키워드 활용: 위 스크립트에서 categories 리스트를 아주 상세하게 확장. (예: \"한국 역사\" -> \"조선 시대 왕의 업적\", \"고구려의 전쟁사\" 등) 질문의 깊이가 깊어질수록 데이터의 질이 나아짐.\n",
    "\n",
    "##### 비용 관리: gpt-4o 보다는 gpt-4o-mini 모델을 추천. 가격이 수십 배 저렴하면서도 일반 상식 데이터 생성 능력은 충분.\n",
    "\n",
    "### 중복 제거 (Deduplication): 생성된 데이터가 수천 개가 되면 \"지구에서 가장 높은 산은?\" 같은 질문이 중복될 수 있음. 파이썬의 set이나 Pandas를 사용하여 instruction 기준 중복을 반드시 제거."
   ],
   "id": "7326131f8f01e5b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 데이터 검증 및 필터링 (품질 관리)\n",
    "대량 생성된 데이터 중에는 모델이 잘못된 정보(Hallucination) 생성 가능.\n",
    "\n",
    "키워드 필터링: 답변에 \"죄송합니다\", \"인공지능으로서\" 등의 단어가 들어간 데이터 삭제.\n",
    "\n",
    "길이 필터링: instruction이 너무 짧거나 output이 지나치게 긴 데이터는 TinyLlama 학습에 방해 되니 제거."
   ],
   "id": "ef36f065c68b413f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 학습 전 최종 데이터 통합\n",
    "스크립트로 만든 synthetic_knowledge_data.jsonl과 기존 수동 작성 데이터 merge"
   ],
   "id": "f9cb390d57d41639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 파일을 읽어서 합치기\n",
    "df1 = pd.read_json(\"manual_data.jsonl\", lines=True)\n",
    "df2 = pd.read_json(\"synthetic_knowledge_data.jsonl\", lines=True)\n",
    "\n",
    "final_df = pd.concat([df1, df2]).drop_duplicates(subset=['instruction'])\n",
    "final_df.to_json(\"total_train_data.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ],
   "id": "11a9f9b27da5d663"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
