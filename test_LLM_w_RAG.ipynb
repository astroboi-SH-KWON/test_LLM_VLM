{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 필수 라이브러리 설치\n",
    "##### LangChain과 벡터 DB Chroma, 임베딩 모델을 위한 라이브러리를 설치\n",
    "\n",
    "##### pip install langchain langchain-community langchain-huggingface chromadb sentence-transformers"
   ],
   "id": "50e24afaf3ed97aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. RAG 구현 코드\n",
    "##### 특정 텍스트 문서(예: 상식 데이터)를 기반으로 TinyLlama가 답변"
   ],
   "id": "d7d0d00cddcf2568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from config import config\n",
    "\n",
    "# 1. 모델 및 임베딩 설정 TinyLlama 모델 로드 (Fine-tuning한 모델 경로)  # # TODO !!!!!!!\n",
    "model_id = config.save_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256, temperature=0.1)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# # TODO HuggingFace에서 한국어 성능 좋은 임베딩 모델 찾기\n",
    "embeddings = HuggingFaceEmbeddings(model_name=config.embedding_path)\n",
    "\n",
    "# 2. 문서 및 벡터DB 설정  TODO 참조할 파일 load 하는 방식으로 수정\n",
    "documents = [\"사과는 비타민 C가 풍부합니다.\", \"대한민국의 수도는 서울입니다.\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. [중요] LCEL 방식의 Chain 구성 (RetrievalQA 대체)\n",
    "template = \"\"\"참고 문맥을 바탕으로 질문에 답하세요:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "답변:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LCEL 파이프라인 구성: 질문 -> 컨텍스트 검색 -> 프롬프트 생성 -> 모델 실행 -> 결과 출력\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4. 실행\n",
    "response = rag_chain.invoke(\"대한민국의 수도는 어디야?\")\n",
    "print(response)"
   ],
   "id": "164bee181ee5b386"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-02T08:59:55.701715Z",
     "start_time": "2026-01-02T08:59:46.006229Z"
    }
   },
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# 1. 문서 준비 (데이터셋이나 텍스트 파일) TODO 참조할 파일 load 하는 방식으로\n",
    "documents = [\n",
    "    \"사과는 비타민 C와 식이섬유가 풍부하여 소화와 면역력 강화에 도움을 줍니다.\",\n",
    "    \"대한민국의 수도는 서울이며, 경복궁과 남산타워가 유명한 명소입니다.\",\n",
    "    \"태양계에서 가장 뜨거운 행성은 금성입니다. 두꺼운 이산화탄소 대기가 열을 가두기 때문입니다.\",\n",
    "    \"파이썬의 len() 함수는 리스트나 문자열의 길이를 반환합니다.\"\n",
    "]\n",
    "\n",
    "# 2. 문서를 벡터화(Embedding)하여 저장소에 저장\n",
    "# # TODO HuggingFace에서 한국어 성능 좋은 임베딩 모델 찾기\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "# 메모리 상에 벡터 데이터베이스 구축\n",
    "vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "\n",
    "# 3. TinyLlama 모델 로드 (Fine-tuning한 모델 경로)  # # TODO !!!!!!!\n",
    "model_id = \"./output/my_llm_final\" # 혹은 \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# HuggingFace 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1, # RAG에서는 정확도가 중요하므로 낮게 설정\n",
    "    top_p=0.95\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. RAG용 프롬프트 템플릿 설정\n",
    "template = \"\"\"### 지시사항: 아래 제공된 참고 문맥을 바탕으로 질문에 답하세요.\n",
    "답을 모른다면 모른다고 대답하고, 답을 지어내지 마세요.\n",
    "\n",
    "### 참고 문맥:\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "### 답변:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# 5. RAG 체인 구축\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}), # 관련 문서 2개 추출\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. 질문 실행\n",
    "query = \"금성이 태양계에서 가장 뜨거운 이유는 뭐야?\"\n",
    "result = qa_chain.invoke(query)\n",
    "\n",
    "print(f\"\\n질문: {query}\")\n",
    "print(f\"결과: {result['result']}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data_analysis/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvectorstores\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Chroma\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_text_splitters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RetrievalQA\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompts\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PromptTemplate\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'langchain.chains'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 최적화 포인트\n",
    "##### 1) Embedding Model (검색의 핵심)\n",
    "###### HuggingFace에서 한국어 성능 좋은 임베딩 모델 찾기 (e.g ko-sroberta-multitask)\n",
    "\n",
    "##### 2) Prompt Engineering (소형 모델의 가이드)\n",
    "###### TinyLlama는 매개변수가 적기 때문에 프롬프트가 매우 중요 ### 참고 문맥:과 같이 명확한 구분자를 주어 모델이 어디를 읽고 어디에 대답해야 하는지 명시.\n",
    "\n",
    "##### 3) Temperature 설정\n",
    "###### RAG에서는 모델의 창의성보다 문서에 기반한 정확성이 중요. 따라서 temperature를 0.1에 가깝게 낮게 설정하여 멋대로 말을 지어내는 '할루시네이션'을 방지.\n",
    "\n",
    "##### 4) 소형 모델의 한계 극복\n",
    "###### TinyLlama가 문맥이 너무 길면 답변 어려움. chunk_size를 200~300 정도로 짧게 유지, k값(가져올 문서 개수)을 2~3개 정도로 제한(for 2080 Ti 메모리와 모델 성능 모두에 유리)"
   ],
   "id": "3d383016802586ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bbeabcf3a742b989"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
