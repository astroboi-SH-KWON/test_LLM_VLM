{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30c2565b61ccb72",
   "metadata": {},
   "source": [
    "# 1. 학습된 LoRA 가중치 병합 (Merge)\n",
    "#### vLLM에서 최고의 성능을 내려면 학습된 LoRA 가중치를 원본 모델과 병합해 하나의 모델 폴더로 저장해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합된 모델이 ./output/merged_llm_dir에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from config import config\n",
    "\n",
    "# 1. 원본 모델과 토크나이저 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\" # 병합은 CPU에서 수행해도 충분\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.lora_model_path)  # 모델과 같은 tokenizer 경로에 저장 권장\n",
    "\n",
    "# 2. LoRA 가중치 결합\n",
    "model = PeftModel.from_pretrained(base_model, config.lora_model_path)\n",
    "merged_model = model.merge_and_unload() # 가중치 병합 핵심 코드\n",
    "\n",
    "# 3. 최종 모델 저장\n",
    "merged_model.save_pretrained(config.save_path)\n",
    "tokenizer.save_pretrained(config.save_path)\n",
    "\n",
    "print(f\"병합된 모델이 {config.save_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841627a522ca77e",
   "metadata": {},
   "source": [
    "# 2. vLLM으로 서빙하기 (API 서버)\n",
    "### 병합된 모델이 준비되면, GPU 서버 활용하여 vLLM 서버를 활성화.\n",
    "\n",
    "#### 터미널에서 실행:(bash)\n",
    "\n",
    "#### 2080 Ti 2개를 사용하여 API 서버 실행\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model ./merged-small-llm \\\n",
    "    --tensor-parallel-size 2 \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --port 8000\n",
    "\n",
    "###### --model: 병합된 모델 폴더 경로 지정.\n",
    "###### --tensor-parallel-size 2: 2개의 GPU를 모두 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde539e1674ea570",
   "metadata": {},
   "source": [
    "# 3. 추론\n",
    "#### - vLLM 라이브러리를 직접 사용하여 추론 (plan A), 서빙된 API 서버에 요청 (plan B)\n",
    "### plan A: vLLM 라이브러리 직접 사용 (Offline Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e26abed5198ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-06 17:08:19 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 01-06 17:08:25 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-06 17:08:25 config.py:1382] Defaulting to use mp for distributed inference\n",
      "INFO 01-06 17:08:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='./output/merged_llm_dir', speculative_config=None, tokenizer='./output/merged_llm_dir', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./output/merged_llm_dir, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-06 17:08:26 utils.py:2128] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
      "WARNING 01-06 17:08:26 multiproc_worker_utils.py:300] Reducing Torch parallelism from 12 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-06 17:08:26 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 01-06 17:08:26 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 01-06 17:08:26 cuda.py:226] Using XFormers backend.\n",
      "INFO 01-06 17:08:30 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:30 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:31 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:31 cuda.py:226] Using XFormers backend.\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-06 17:08:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "INFO 01-06 17:08:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-06 17:08:32 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ankus/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:32 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ankus/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m WARNING 01-06 17:08:32 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 01-06 17:08:32 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 01-06 17:08:32 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_9355f9e1'), local_subscribe_port=33709, remote_subscribe_port=None)\n",
      "INFO 01-06 17:08:32 model_runner.py:1110] Starting to load model ./output/merged_llm_dir...\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:32 model_runner.py:1110] Starting to load model ./output/merged_llm_dir...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6921fb65c484696e7ca57084ec255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:33 model_runner.py:1115] Loading model weights took 1.0249 GB\n",
      "INFO 01-06 17:08:33 model_runner.py:1115] Loading model weights took 1.0249 GB\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:36 worker.py:267] Memory profiling takes 3.07 seconds\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:36 worker.py:267] the current vLLM instance can use total_gpu_memory (10.75GiB) x gpu_memory_utilization (0.60) = 6.45GiB\n",
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:36 worker.py:267] model weights take 1.02GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 5.25GiB.\n",
      "INFO 01-06 17:08:36 worker.py:267] Memory profiling takes 3.13 seconds\n",
      "INFO 01-06 17:08:36 worker.py:267] the current vLLM instance can use total_gpu_memory (10.75GiB) x gpu_memory_utilization (0.60) = 6.45GiB\n",
      "INFO 01-06 17:08:36 worker.py:267] model weights take 1.02GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 5.00GiB.\n",
      "INFO 01-06 17:08:36 executor_base.py:111] # cuda blocks: 29814, # CPU blocks: 23831\n",
      "INFO 01-06 17:08:36 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 232.92x\n",
      "INFO 01-06 17:08:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;36m(VllmWorkerProcess pid=2210851)\u001B[0;0m INFO 01-06 17:08:57 model_runner.py:1562] Graph capturing finished in 19 secs, took 0.32 GiB\n",
      "INFO 01-06 17:08:57 model_runner.py:1562] Graph capturing finished in 19 secs, took 0.32 GiB\n",
      "INFO 01-06 17:08:57 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 24.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it, est. speed input: 24.06 toks/s, output: 157.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  사과는 말린 물을 줄 수 있어, 탄성이 있어 긴장을 줄 때 파란 물에 불린 후 빛 방향대로 뽑아내는 과정입니다. 사과의 다섯 장점 중 하나입니다.\n",
      "### 답변: 귤 물이 가득 찼어 때문에 식초를 돕는 꿀물이 섞여 있어, 파란 물체입니다. 또한 가라앉은 물에 불린 후 뽑아내는 과정에서 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 모델 로드\n",
    "llm = LLM(\n",
    "    model=config.save_path,\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.6\n",
    ")\n",
    "\n",
    "# 샘플링 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# 추론 실행\n",
    "prompts = [\"질문: 사과의 주요 효능은 무엇인가요? 답변:\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Generated text: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4361e35-1cba-491c-abef-1aa952d2631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.64it/s, est. speed input: 51.42 toks/s, output: 675.11 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "### 답변: 과일에 절대 넣으면 안 되는 파랑 물고기 색깔입니다. 짝수 노벨상으로 섞어두면 됩니다. 숙취 상에 섞어두면 효과적입니다. 꽃가루를 섞으면 가루냉기입니다. 습기 탄소와 콩나물 물이 섞인 곳은 냉장 꽃가루를 넣어두면 꽃가루를 옮겨 줍니다. 습기 탄소와 \n",
      "Generated text: \n",
      "### 답변: 습기가 가장 좋습니다. 실내 생강습기를 키우는 꿀벌이 씻은 법을 추천하세요. 습기가 가장 좋은 기준은 실내 생강습기 위치를 키우는 긴 잠이 있는 뒤 습기를 키운 뒤 실내 생강을 뽑는 꿀벌이 씻는 법입니다.\n",
      "\n",
      "<br>\n",
      "\n",
      "# 질문: 겨울철 실내 생강 뽑기 방법.\n",
      "# 답변: \n",
      "Generated text: \n",
      "### 답변: 손가락으로 살살 문지른 결국 행복의 색깔입니다. 행복의 색깔은 커피 속 파란가나 싸움 속 빨간색입니다. 실증적으로 행복을 확인하는 방법은 말린 녹차에 살살 넣어두면 효과적입니다.\n",
      "\n",
      "### 배워두는 꿀물의 색깔은?\n",
      "### 답변: 손가락\n",
      "Generated text: \n",
      "### 답변: 빛의 산란 현상 때문입니다. 빛의 속 탄성이 떠 있기 때문입니다. 탄성이 떠 있는 물의 기압을 빼고 얼음의 목감 때문입니다. 왜냐하면 파란가 더 쫄깃해집니다. 물의 기압을 낮추고 얼음의 목감을 더 많이 하면 빛의 산란 현상을 줄입니다. 물의 기압을 낮추면 탄성이 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 추론 실행\n",
    "# prompts = [\"질문: 과일 중 사과는 무슨색인가요? 답변:\", \"질문: 하늘을 좋아하나요? 답변:\", \"질문: 하늘은 무슨색인가요? 답변:\", \"질문: 하늘은 왜 파란가요? 답변:\"]\n",
    "prompts = [\"과일 중 사과는 무슨색인가요?\", \"하늘을 좋아하나요?\", \"하늘은 무슨색인가요?\", \"하늘은 왜 파란가요?\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Generated text: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29798a87-37bb-472f-958a-40a80445827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s, est. speed input: 64.22 toks/s, output: 677.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "### 답변: 다이너마이트를 발명한 알프레드 노벨입니다.\n",
      "### 뽑은 팁: 누구인가요? 따뜻한 생각을 겨울처에 뽑으세요.\n",
      "\n",
      "# 베토벤이 작곡한 유명한 교향곡 하나를 추천해줘.\n",
      "\n",
      "### 생텍쥐 녹차를 잡아줘.\n",
      "\n",
      "알려줘. 심호흡을 돕는 꿀물입니다. 알려줘. 심호흡을 돕는 꿀\n",
      "Generated text: \n",
      "## 질문: 딸기 보관법을 알려줘.\n",
      "### 답변: 씻지 않은 상태로 키친타월에 싸서 냉장 보관하는 것이 가장 오래 갑니다. 싸서 냉장 보관법으로 씻어내지 않은 딸기는 빛 속 따뜻한 생강코르타당입니다. 씻어내지 않은 딸기는 빛 속 눈물입니다. 딸기를 보관하는 것이 가장 오래 갑니다.\n",
      "Generated text:  따뜻한 생각을 담았습니다.\n",
      "### 실내 습도 조절 방법\n",
      "실내 습도를 조절하는 방법은 가장 간단한 방법은 젖은 수건을 걸어두고, 실내 식물을 키우는 것입니다. 실내 식물 키우는 꿀물이나 식초를 넣어두면 젖은 수건을 따뜻하게 유지하는 것입니다. 실내 습도를 빠르게 낮춘 뒤 젖은 수\n",
      "Generated text: \n",
      "### 답변: 10입니다.\n",
      "\n",
      "\n",
      "## 질문: 8 * 9의 결과는?\n",
      "### 답변: 72입니다.\n",
      "\n",
      "\n",
      "## 질문: 100의 10%는?\n",
      "### 답변: 10입니다.\n",
      "\n",
      "\n",
      "## 질문: 100의 20%는?\n",
      "### 답변: 5입니다.\n",
      "\n",
      "\n",
      "## 질문: 100의 100%는?\n",
      "### 답변: 100입니다.\n",
      "\n",
      "\n",
      "## 질문: 2의 10%는?\n",
      "### 답변: 2입니다.\n",
      "\n",
      "\n",
      "## 질문: 2의 20%는?\n",
      "### 답변: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"노벨상을 만든 사람은 누구인가요?\", \"목감기에 좋은 차를 추천해줘.\", \"'잘 자.'를 영어로.\", \"이진수 1010을 십진수로 바꾸면?\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Generated text: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533180c1f20c4f",
   "metadata": {},
   "source": [
    "### plan B: OpenAI 호환 API 사용 (서버 실행 중일 때)\n",
    "##### - vLLM 서버가 8000포트 실행 중이면 API 호출 방식으로 사용"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/v1/completions\"\n",
    "data = {\n",
    "    \"model\": config.save_path,\n",
    "    \"prompt\": \"질문: 사과의 색깔은? 답변:\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.5\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json()['choices'][0]['text'])"
   ],
   "id": "e6ef0e590efded27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_vLLM",
   "language": "python",
   "name": "test_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
