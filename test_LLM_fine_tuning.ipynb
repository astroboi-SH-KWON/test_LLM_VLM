{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a449ede8d98f3",
   "metadata": {},
   "source": [
    "# 1. 학습 방식 선택\n",
    "### 작은 모델(예: 1B~3B 파라미터)을 학습시킬 때\n",
    "#### - Pre-training: 아무것도 모르는 모델에게 대량의 텍스트를 읽혀 언어 구조를 배우게 함.\n",
    "#### - Fine-tuning: 이미 학습된 모델(예: EXAONE-1.2B, TinyLlama)에 특정 도메인 데이터를 추가로 학습시킴.\n",
    "\n",
    "# 2. 환경 준비 및 데이터셋 구성\n",
    "#### - 학습을 위해 모델이 읽을 수 있는 형태로 데이터를 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3f5fbf-9e26-49a3-8a08-3122ae761978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 52개의 데이터가 tinyllama_train_data.jsonl로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = [\n",
    "    # --- 일반 상식 및 정보 (20개) ---\n",
    "    {\"instruction\": \"지구에서 가장 높은 산은 무엇인가요?\", \"output\": \"지구에서 해발 고도가 가장 높은 산은 에베레스트 산입니다.\"},\n",
    "    {\"instruction\": \"태양계에서 가장 큰 행성은?\", \"output\": \"태양계에서 가장 큰 행성은 목성입니다.\"},\n",
    "    {\"instruction\": \"광합성이란 무엇인가요?\", \"output\": \"광합성은 식물이 빛 에너지를 이용하여 이산화탄소와 물로부터 유기물을 합성하는 과정입니다.\"},\n",
    "    {\"instruction\": \"대한민국의 국보 1호는?\", \"output\": \"대한민국의 국보 제1호는 숭례문입니다.\"},\n",
    "    {\"instruction\": \"세종대왕의 업적 중 가장 대표적인 것은?\", \"output\": \"가장 대표적인 업적은 백성들이 쉽게 글을 익힐 수 있도록 '훈민정음(한글)'을 창제하신 것입니다.\"},\n",
    "    {\"instruction\": \"물의 화학 기호는 무엇인가요?\", \"output\": \"물의 화학 기호는 H2O입니다.\"},\n",
    "    {\"instruction\": \"베토벤이 작곡한 유명한 교향곡 하나를 추천해줘.\", \"output\": \"운명 교향곡으로도 불리는 제5번 교향곡을 추천합니다.\"},\n",
    "    {\"instruction\": \"프랑스의 수도는 어디인가요?\", \"output\": \"프랑스의 수도는 파리입니다.\"},\n",
    "    {\"instruction\": \"비타민 C가 많이 들어있는 음식은?\", \"output\": \"귤, 오렌지, 키위, 고추, 브로콜리 등에 비타민 C가 풍부합니다.\"},\n",
    "    {\"instruction\": \"소설 '어린 왕자'의 저자는 누구인가요?\", \"output\": \"생텍쥐페리입니다.\"},\n",
    "    {\"instruction\": \"사과의 색깔은 무엇인가요?\", \"output\": \"사과는 보통 빨간색이나 초록색입니다.\"},\n",
    "    {\"instruction\": \"하늘은 왜 파란가요?\", \"output\": \"빛의 산란 현상 때문입니다.\"}, \n",
    "\n",
    "    # --- 실용적인 가이드 및 조언 (30개) ---\n",
    "    {\"instruction\": \"컴퓨터 속도를 빠르게 하는 법 3가지.\", \"output\": \"1. 불필요한 프로그램 삭제 2. 시작 프로그램 정리 3. SSD 여유 공간 확보.\"},\n",
    "    {\"instruction\": \"잠이 잘 오는 방법을 알려줘.\", \"output\": \"규칙적인 수면 시간을 유지하고, 취침 전 카페인 섭취를 피하며 따뜻한 우유를 마시는 것이 도움이 됩니다.\"},\n",
    "    {\"instruction\": \"여권 발급 시 필요한 준비물은?\", \"output\": \"신분증, 최근 6개월 이내 촬영한 여권용 사진 1매, 수수료가 필요합니다.\"},\n",
    "    {\"instruction\": \"운동 전 스트레칭의 장점은?\", \"output\": \"근육의 긴장을 풀고 유연성을 높여 부상을 예방해 줍니다.\"},\n",
    "    {\"instruction\": \"효과적인 공부법 하나만 알려줘.\", \"output\": \"학습한 내용을 남에게 가르치듯이 말해보는 '인출 연습'이 매우 효과적입니다.\"},\n",
    "    {\"instruction\": \"면접에서 좋은 인상을 남기는 법.\", \"output\": \"밝은 미소로 인사하고, 질문의 의도를 정확히 파악하여 결론부터 대답하는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"겨울철 실내 습도 조절 방법.\", \"output\": \"가습기를 사용하거나 젖은 수건을 걸어두고, 실내 식물을 키우는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"맛있는 라면 끓이는 비법은?\", \"output\": \"물이 끓기 전에 스프를 먼저 넣고, 면을 들었다 놨다 하며 공기와 접촉시키면 면발이 더 쫄깃해집니다.\"},\n",
    "    {\"instruction\": \"커피 얼룩 지우는 방법.\", \"output\": \"주방세제와 식초를 섞어 얼룩 부위를 살살 문지른 후 미온수로 헹궈내세요.\"},\n",
    "    {\"instruction\": \"화분 물 주는 시기 확인법.\", \"output\": \"손가락으로 겉흙을 2cm 정도 찔러보았을 때 속까지 말라 있으면 물을 줄 때입니다.\"},\n",
    "\n",
    "    # --- 언어 작업: 요약, 번역, 문장 변환 (20개) ---\n",
    "    {\"instruction\": \"'밥 먹었니?'를 존댓말로 바꿔줘.\", \"output\": \"식사하셨어요?\"},\n",
    "    {\"instruction\": \"'I love you'를 한국어로 번역해.\", \"output\": \"사랑해 또는 사랑합니다.\"},\n",
    "    {\"instruction\": \"다음 문장을 요약해: 인공지능 기술이 발전함에 따라 우리의 일상은 편리해졌지만 사생활 침해 우려도 커지고 있다.\", \"output\": \"AI 발전으로 일상은 편리해졌으나 사생활 침해 문제가 제기되고 있다.\"},\n",
    "    {\"instruction\": \"'슬프다'의 반대말은?\", \"output\": \"기쁘다입니다.\"},\n",
    "    {\"instruction\": \"'노력은 배신하지 않는다'는 속담의 뜻은?\", \"output\": \"무엇이든 열심히 하면 반드시 좋은 결과를 얻게 된다는 뜻입니다.\"},\n",
    "    {\"instruction\": \"다음 단어들을 사용해 한 문장을 만들어줘: 강아지, 공원, 산책.\", \"output\": \"강아지와 함께 공원에서 즐겁게 산책을 했습니다.\"},\n",
    "    {\"instruction\": \"감동적인 이별 노래 제목 3개 추천해줘.\", \"output\": \"1. 안녕 (폴킴) 2. 보고 싶다 (김범수) 3. 서른 즈음에 (김광석).\"},\n",
    "    {\"instruction\": \"'고맙습니다'를 영어로 번역하면?\", \"output\": \"Thank you입니다.\"},\n",
    "    {\"instruction\": \"'사과'라는 단어로 짧은 2행시를 지어줘.\", \"output\": \"사: 사시사철 먹어도 맛있는 과일 / 과: 과연 사과가 최고야.\"},\n",
    "    {\"instruction\": \"어색한 문장을 수정해줘: 나는 학교에 갔고 밥을 먹었다 하지만 졸렸다.\", \"output\": \"나는 학교에 가서 밥을 먹었지만 졸음이 쏟아졌다.\"},\n",
    "\n",
    "    # --- 창의적 글쓰기 및 상황극 (20개) ---\n",
    "    {\"instruction\": \"비 오는 날 어울리는 점심 메뉴 제안해줘.\", \"output\": \"비 오는 날에는 따뜻한 국물이 있는 칼국수나 노릇하게 구운 파전을 추천합니다.\"},\n",
    "    {\"instruction\": \"새해 인사말 문구 작성해줘.\", \"output\": \"새해 복 많이 받으시고, 올 한 해 소망하시는 모든 일이 이루어지길 기원합니다.\"},\n",
    "    {\"instruction\": \"생일 축하 편지 첫 줄을 써줘.\", \"output\": \"세상에서 가장 특별한 너의 생일을 진심으로 축하해!\"},\n",
    "    {\"instruction\": \"여행 가방 쌀 때 잊지 말아야 할 것.\", \"output\": \"휴대폰 충전기와 보조배터리, 그리고 비상약은 꼭 챙기세요.\"},\n",
    "    {\"instruction\": \"고민 상담: 공부가 너무 하기 싫을 때 어떡해?\", \"output\": \"딱 10분만 앉아있자고 자신과 약속해보세요. 시작이 가장 어렵답니다.\"},\n",
    "    {\"instruction\": \"오늘의 행운 메시지를 적어줘.\", \"output\": \"당신이 미소 지을 때 행운은 당신 곁으로 다가옵니다. 오늘 하루도 웃음 가득하세요!\"},\n",
    "    {\"instruction\": \"친구에게 빌린 책을 돌려줄 때 쓰는 메모.\", \"output\": \"덕분에 정말 재미있게 읽었어! 빌려줘서 고마워.\"},\n",
    "    {\"instruction\": \"이사 가는 친구에게 응원 메시지.\", \"output\": \"새로운 집에서 기분 좋은 일들만 가득하길 바랄게! 이사 축하해.\"},\n",
    "    {\"instruction\": \"카페 사장님이 손님에게 할 수 있는 인사.\", \"output\": \"어서 오세요! 따뜻하고 향긋한 커피 한 잔과 함께 편안한 시간 보내세요.\"},\n",
    "    {\"instruction\": \"우울할 때 들으면 좋은 말.\", \"output\": \"지금 충분히 잘하고 있어. 조금 쉬어가도 괜찮아.\"},\n",
    "\n",
    "    # --- 논리 및 수학/코딩 (10개) ---\n",
    "    {\"instruction\": \"15 + 27의 결과는?\", \"output\": \"42입니다.\"},\n",
    "    {\"instruction\": \"사각형의 넓이를 구하는 공식은?\", \"output\": \"가로 길이 x 세로 길이입니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 'Hello World'를 출력하는 코드는?\", \"output\": \"print('Hello World')\"},\n",
    "    {\"instruction\": \"100의 10%는 얼마인가요?\", \"output\": \"10입니다.\"},\n",
    "    {\"instruction\": \"반지름이 5인 원의 넓이는? (원주율은 3.14)\", \"output\": \"넓이는 78.5입니다. (5 * 5 * 3.14)\"},\n",
    "    {\"instruction\": \"3의 3제곱은?\", \"output\": \"27입니다.\"},\n",
    "    {\"instruction\": \"짝수란 무엇인가요?\", \"output\": \"2로 나누어떨어지는 정수를 말합니다.\"},\n",
    "    {\"instruction\": \"자바스크립트에서 현재 시간을 가져오는 코드는?\", \"output\": \"new Date()\"},\n",
    "    {\"instruction\": \"가장 작은 소수는 무엇인가요?\", \"output\": \"가장 작은 소수는 2입니다.\"},\n",
    "    {\"instruction\": \"HTML에서 하이퍼링크를 만드는 태그는?\", \"output\": \"<a> 태그를 사용합니다.\"}\n",
    "]\n",
    "\n",
    "# 실제 100개를 채우기 위해 위 데이터를 변형하거나 확장하여 저장 (여기서는 대표 예시 50개를 보여드립니다)\n",
    "# 전체 데이터 100개를 루프를 통해 파일로 저장하는 로직\n",
    "with open(\"./input/tinyllama_train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data: # 100개 데이터라고 가정\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"총 {len(data)}개의 데이터가 tinyllama_train_data.jsonl로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a8588d-3a54-4616-ada8-092b34d1322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankus/miniconda3/envs/test_vLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 52 examples [00:00, 22536.04 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 2659.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료된 데이터 샘플: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 1. 데이터셋 로드\n",
    "# 지시하신 jsonl 파일을 불러옵니다.\n",
    "dataset = Dataset.from_json(\"./input/tinyllama_train_data.jsonl\")\n",
    "\n",
    "# 2. 모델 및 토크나이저 로드\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.padding_side = \"right\" # 학습 시에는 오른쪽 패딩 권장\n",
    "\n",
    "# 3. 데이터 전처리 함수 수정\n",
    "def tokenize_function(examples):\n",
    "    # TinyLlama가 이해하기 쉬운 프롬프트 형태로 합칩니다.\n",
    "    # 포맷: ### 질문: ... \\n### 답변: ... <|endoftext|>\n",
    "    texts = [\n",
    "        f\"### 질문: {inst}\\n### 답변: {out}{tokenizer.eos_token}\"\n",
    "        for inst, out in zip(examples[\"instruction\"], examples[\"output\"])\n",
    "    ]\n",
    "    \n",
    "    # 토큰화 수행\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# 전처리 적용 (기존 instruction, output 컬럼은 제거)\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"instruction\", \"output\"]\n",
    ")\n",
    "\n",
    "print(f\"전처리 완료된 데이터 샘플: {tokenized_datasets[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:30:45.077866Z",
     "start_time": "2025-12-31T05:30:36.262867Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# # 1. 예시 데이터셋 생성 TODO .txt나 .json 파일을 로드하는 방식으로 수정 필요\n",
    "# data = [\n",
    "#     {\"instruction\": \"사과의 주요 효능 3가지를 알려줘.\", \"output\": \"사과의 주요 효능은 다음과 같습니다. 1. 식이섬유가 풍부하여 소화를 돕습니다. 2. 비타민 C가 함유되어 면역력을 강화합니다. 3. 항산화 성분이 노화 방지에 도움을 줍니다.\"},\n",
    "#     {\"instruction\": \"파이썬에서 리스트의 길이를 구하는 함수는?\", \"output\": \"파이썬에서 리스트의 길이를 구하는 함수는 len()입니다.\"},\n",
    "#     {\"instruction\": \"여름철 수박을 고르는 꿀팁을 간단히 요약해봐.\", \"output\": \"수박은 두드렸을 때 맑은 소리가 나고, 배꼽 부분이 작으며, 껍질의 줄무늬가 선명한 것을 고르는 것이 좋습니다.\"},\n",
    "#     # {\"text\": \"질문: 사과의 색깔은 무엇인가요? 답변: 사과는 보통 빨간색이나 초록색입니다.\"},\n",
    "#     # {\"text\": \"질문: 하늘은 왜 파란가요? 답변: 빛의 산란 현상 때문입니다.\"},\n",
    "# ]\n",
    "# # # 텍스트 데이터를 토큰화 by Dataset\n",
    "# dataset = Dataset.from_list(data)\n",
    "\n",
    "# # 2. 모델 및 토크나이저 로드 (작은 모델: TinyLlama)\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token # 패딩 토큰 설정\n",
    "\n",
    "# # 3. 데이터 전처리 함수\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12529b072f07b4",
   "metadata": {},
   "source": [
    "# 3. 좆밥 GPU 위한 효율적인 학습 기법 (LoRA)\n",
    "#### - GPU 메모리(예: 11GB 내외의 RTX 2080 Ti)가 부족하여, 모델 전체를 학습시키는 대신 일부 파라미터만 학습시키는 LoRA(Low-Rank Adaptation) 방식 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80773b71e7bb59a2",
   "metadata": {},
   "source": [
    "#### 1) (가장 중요) 가중치 정밀도 및 로드 설정 (Quantization)\n",
    "###### 모델 자체의 무게를 줄여 GPU의 기본 점유 공간을 확보하는 것이 가장 중요\n",
    "##### - load_in_4bit=True (QLoRA): 모델을 4비트로 양자화하여 로드. 16비트 대비 메모리 사용량 약 1/4로 줄임. 20280ti 11GB 메모리라면 7B 모델도 간신히 올릴 수 있을까?\n",
    "###### - bnb_4bit_compute_dtype=torch.bfloat16: 연산 시의 정밀도 설정. float16보다 bfloat16이 학습 안정성이 높지만, 구형 GPU(2080 Ti 등)는 float16을 사용해야 그나마 속도 나옴.\n",
    "###### - bnb_4bit_quant_type=\"nf4\": 일반적인 4비트보다 더 정밀한 NormalFloat4 방식을 사용하여 양자화로 인한 성능 저하를 최소화\n",
    "\n",
    "#### 2) LoRA 하이퍼파라미터 (PEFT Config)\n",
    "###### 학습 대상이 되는 파라미터의 수 결정.\n",
    "###### - r (Rank): LoRA 행렬의 크기. 값이 클수록 표현력은 좋아지지만 메모리 사용량 커짐. 보통 8 또는 16을 권장, 메모리가 극도로 부족하면 4까지 도전!!!\n",
    "###### - target_modules: 학습할 레이어를 지정. [\"q_proj\", \"v_proj\"]만 지정하면 메모리를 아낄 수 있고, [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]처럼 모두 지정하면 성능은 좋아지지만 메모리 부하 큼.\n",
    "###### - task_type=\"CAUSAL_LM\": 텍스트 생성 모델의 경우 이 설정을 명시하여 불필요한 레이어가 학습되는 것을 방지.\n",
    "\n",
    "#### 3) 배치 사이즈 및 그래디언트 설정 (Training Arguments)\n",
    "###### 학습 데이터가 GPU를 거칠 때 발생하는 부하를 조절.\n",
    "###### - per_device_train_batch_size=1: 실제 GPU에 한 번에 올리는 데이터 개수. !!! OOM이 발생하면 무조건 1로 설정 !!!\n",
    "###### - gradient_accumulation_steps: 부족한 배치 사이즈를 보완하는 핵심 변수. 예를 들어 배치 1로 설정하고 이 값을 16으로 주면, 16번의 계산 결과를 모아서 한 번 가중치를 업데이트. 실제 배치 사이즈 16의 효과를 내면서 메모리는 1만큼만 사용(혜자닷!).\n",
    "##### - gradient_checkpointing=True: 역전파(Backpropagation) 과정에서 필요한 중간 계산값을 모두 저장하지 않고, 필요할 때 다시 계산. 연산 속도는 약 30% 느려지지만 메모리 점유율을 획기적으로 낮춤(약간 대출 느낌?).\n",
    "\n",
    "#### 4) 시퀀스 길이 조절 (Data Processing)\n",
    "###### 데이터 한 건의 길이가 길수록 메모리 사용량은 기하급수적으로 늘어남.\n",
    "##### - max_seq_length (또는 max_length): 모델이 한 번에 처리하는 토큰 수. 기본 2048, 4096, 메모리가 부족하면 512나 1024로.\n",
    "###### - group_by_length=True: 길이가 비슷한 데이터끼리 묶어서 배치 학습을 진행. 패딩(Padding) 토큰 낭비를 줄여 효율을 높임.\n",
    "\n",
    "#### 5) 최적화 도구 (Optimizer & Offloading)\n",
    "###### 학습 시 사용되는 수학적 최적화 도구의 메모리 점유를 줄임.\n",
    "##### - optim=\"paged_adamw_8bit\" 또는 \"paged_adamw_32bit\": 최적화 도구의 상태값을 8비트로 압축하거나, 필요 없는 데이터를 CPU로 잠시 넘기는(Paged) 기능을 활성화. 일반 AdamW보다 메모리를 훨씬 적게 사용.\n",
    "###### - fp16=True: 16비트 혼합 정밀도 학습을 사용하여 32비트 대비 메모리 사용량을 절반으로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2783c7076b2ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:42:03.453416Z",
     "start_time": "2025-12-31T05:30:45.094698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    # 100개 데이터라면 표현력을 높이기 위해 8보다 16을 추천 (메모리 허용 시)\n",
    "    r=16,  # # r(Rank): LoRA 행렬의 크기. 값이 클수록 표현력은 좋아지지만 메모리 사용량 커짐. 보통 8 또는 16을 권장, 메모리가 극도로 부족하면 4까지 도전!!!\n",
    "    lora_alpha=32,\n",
    "    # [수정] q, v 뿐만 아니라 모든 리니어 레이어를 타겟으로 잡아 학습 효율 극대화.\n",
    "    # 소형 모델일수록 더 많은 레이어를 학습하는 것이 한국어 습득에 유리.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    lora_dropout=0.1,    # 데이터가 적으므로 드롭아웃을 살짝 높여 과적합을 방지.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters() # 학습 가능한 파라미터 비중 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5286ea8a54898",
   "metadata": {},
   "source": [
    "# 4. 학습 실행 (Trainer API)\n",
    "### Hugging Face의 Trainer를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fa322093e6a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./output/my_llm/tokenizer_config.json',\n",
       " './output/my_llm/special_tokens_map.json',\n",
       " './output/my_llm/tokenizer.model',\n",
       " './output/my_llm/added_tokens.json',\n",
       " './output/my_llm/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "\n",
    "    # 1. 학습 루프 설정\n",
    "    num_train_epochs=10,             # 전체 데이터 반복 횟수  # # 100개 데이터 기준 5~10회 추천\n",
    "    learning_rate=2e-4,             # 학습률\n",
    "    lr_scheduler_type=\"cosine\",       # 학습률을 서서히 줄여 안정적으로 수렴\n",
    "    warmup_ratio=0.1,                 # 초기 10% 구간은 학습률을 서서히 올림\n",
    "    \n",
    "    # 2. 배치 사이즈 및 메모리 최적화 (2080 Ti 11GB 기준)\n",
    "    per_device_train_batch_size=1,   # 메모리 상황에 따라 조절  # GPU 한 장당 배치 1 (OOM 방지)  1 * 4 = 16\n",
    "    gradient_accumulation_steps=4,  # 실제 배치 사이즈 = 4 * 4 = 16 (per_device_train_batch_size=4 인 경우)\n",
    "    gradient_checkpointing=True,      # 메모리 사용량 획기적 절감 (필수)\n",
    "\n",
    "    # 3. 정밀도 및 하드웨어 가속\n",
    "    fp16=True,                      # 16비트 혼합 정밀도 학습 (속도 향상)\n",
    "    # [수정] 2080 Ti라면 8bit 옵티마이저가 메모리 관리에 더 유리\n",
    "    optim=\"paged_adamw_8bit\",       # 메모리 효율적인 옵티마이저  paged_adamw_8bit, paged_adamw_32bit\n",
    "\n",
    "    # 4. 로그 및 저장 전략\n",
    "    logging_steps=5,  # 데이터가 적으므로 더 자주 로그 확인\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,               # 디스크 용량 관리를 위해 최신 체크포인트만 유지\n",
    "\n",
    "    # 5. 성능 최적화\n",
    "    max_grad_norm=1.0,                # 0.3은 너무 엄격할 수 있어 1.0으로 완화하여 학습 속도 개선\n",
    "    weight_decay=0.05,                # 데이터가 적을 때 과적합 방지를 위해 조금 더 강화\n",
    "    group_by_length=True,             # 속도 향상을 위해 길이가 비슷한 데이터끼리 묶음\n",
    ")\n",
    "\n",
    "# 4-1. 그래디언트 체크포인팅 사용 시 필수 설정\n",
    "if getattr(training_args, \"gradient_checkpointing\", False):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    # 아래 설정이 누락되면 해당 에러가 발생 가능\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "# 4-2. LoRA 파라미터가 제대로 잡혔는지 확인 (중요)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 5. 트레이너 초기화 및 학습 시작\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 6. 학습된 모델 저장\n",
    "model.save_pretrained(\"./output/my_llm\")\n",
    "tokenizer.save_pretrained(\"./output/my_llm\") # 모델과 같은 경로에 저장 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86ee984ddf1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_vLLM",
   "language": "python",
   "name": "test_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
