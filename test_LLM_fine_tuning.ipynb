{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a449ede8d98f3",
   "metadata": {},
   "source": [
    "# 1. 학습 방식 선택\n",
    "### 작은 모델(예: 1B~3B 파라미터)을 학습시킬 때\n",
    "#### - Pre-training: 아무것도 모르는 모델에게 대량의 텍스트를 읽혀 언어 구조를 배우게 함.\n",
    "#### - Fine-tuning: 이미 학습된 모델(예: EXAONE-1.2B, TinyLlama)에 특정 도메인 데이터를 추가로 학습시킴.\n",
    "\n",
    "# 2. 환경 준비 및 데이터셋 구성\n",
    "#### - 학습을 위해 모델이 읽을 수 있는 형태로 데이터를 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3f5fbf-9e26-49a3-8a08-3122ae761978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 152개의 데이터가 tinyllama_train_data.jsonl로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = [\n",
    "    # --- 일반 상식 및 정보 ---\n",
    "    {\"instruction\": \"지구에서 가장 높은 산은 무엇인가요?\", \"output\": \"지구에서 해발 고도가 가장 높은 산은 에베레스트 산입니다.\"},\n",
    "    {\"instruction\": \"태양계에서 가장 큰 행성은?\", \"output\": \"태양계에서 가장 큰 행성은 목성입니다.\"},\n",
    "    {\"instruction\": \"광합성이란 무엇인가요?\", \"output\": \"광합성은 식물이 빛 에너지를 이용하여 이산화탄소와 물로부터 유기물을 합성하는 과정입니다.\"},\n",
    "    {\"instruction\": \"대한민국의 국보 1호는?\", \"output\": \"대한민국의 국보 제1호는 숭례문입니다.\"},\n",
    "    {\"instruction\": \"세종대왕의 업적 중 가장 대표적인 것은?\", \"output\": \"가장 대표적인 업적은 백성들이 쉽게 글을 익힐 수 있도록 '훈민정음(한글)'을 창제하신 것입니다.\"},\n",
    "    {\"instruction\": \"물의 화학 기호는 무엇인가요?\", \"output\": \"물의 화학 기호는 H2O입니다.\"},\n",
    "    {\"instruction\": \"베토벤이 작곡한 유명한 교향곡 하나를 추천해줘.\", \"output\": \"운명 교향곡으로도 불리는 제5번 교향곡을 추천합니다.\"},\n",
    "    {\"instruction\": \"프랑스의 수도는 어디인가요?\", \"output\": \"프랑스의 수도는 파리입니다.\"},\n",
    "    {\"instruction\": \"비타민 C가 많이 들어있는 음식은?\", \"output\": \"귤, 오렌지, 키위, 고추, 브로콜리 등에 비타민 C가 풍부합니다.\"},\n",
    "    {\"instruction\": \"소설 '어린 왕자'의 저자는 누구인가요?\", \"output\": \"생텍쥐페리입니다.\"},\n",
    "    {\"instruction\": \"사과의 색깔은 무엇인가요?\", \"output\": \"사과는 보통 빨간색이나 초록색입니다.\"},\n",
    "    {\"instruction\": \"하늘은 왜 파란가요?\", \"output\": \"빛의 산란 현상 때문입니다.\"}, \n",
    "    {\"instruction\": \"가장 가벼운 원소는 무엇인가요?\", \"output\": \"원소 기호 H인 수소가 가장 가벼운 원소입니다.\"},\n",
    "    {\"instruction\": \"인간의 뼈는 총 몇 개인가요?\", \"output\": \"성인 기준으로 인간의 뼈는 약 206개입니다.\"},\n",
    "    {\"instruction\": \"세계에서 가장 긴 강은?\", \"output\": \"아프리카 대륙의 나일 강이 세계에서 가장 긴 강으로 알려져 있습니다.\"},\n",
    "    {\"instruction\": \"조선 시대에 측우기를 발명한 사람은?\", \"output\": \"장영실이 세종대왕의 명을 받아 발명하였습니다.\"},\n",
    "    {\"instruction\": \"노벨상을 만든 사람은 누구인가요?\", \"output\": \"다이너마이트를 발명한 알프레드 노벨입니다.\"},\n",
    "    {\"instruction\": \"지구의 위성은 무엇인가요?\", \"output\": \"지구의 자연 위성은 달 하나입니다.\"},\n",
    "    {\"instruction\": \"판소리 5마당을 나열해줘.\", \"output\": \"춘향가, 심청가, 흥보가, 수궁가, 적벽가입니다.\"},\n",
    "    {\"instruction\": \"미국의 독립 기념일은 언제인가요?\", \"output\": \"매년 7월 4일입니다.\"},\n",
    "    {\"instruction\": \"나비의 성장 과정을 설명해줘.\", \"output\": \"알, 애벌레, 번데기, 성충(나비)의 과정을 거칩니다.\"},\n",
    "    {\"instruction\": \"에펠탑이 위치한 도시는?\", \"output\": \"프랑스 파리에 위치하고 있습니다.\"},\n",
    "    {\"instruction\": \"인류 최초로 달에 착륙한 사람은 누구인가요?\", \"output\": \"1969년 아폴로 11호를 타고 달에 착륙한 닐 암스트롱입니다.\"},\n",
    "    {\"instruction\": \"비행기가 하늘을 날 수 있게 하는 원리는?\", \"output\": \"날개 위아래의 기압 차이로 발생하는 '양력' 덕분입니다.\"},\n",
    "    {\"instruction\": \"올림픽 오륜기의 다섯 가지 색깔은 무엇인가요?\", \"output\": \"파랑, 노랑, 검정, 초록, 빨강입니다.\"},\n",
    "    {\"instruction\": \"우리나라에서 가장 긴 터널은?\", \"output\": \"도로 터널 기준으로는 인제양양터널, 철도 터널 기준으로는 율현터널입니다.\"},\n",
    "    {\"instruction\": \"세계에서 인구가 가장 많은 나라는?\", \"output\": \"2023년 통계 기준 인도입니다.\"},\n",
    "    {\"instruction\": \"화폐 단위 '원'의 기호는 어떻게 쓰나요?\", \"output\": \"한글 '원'의 첫 자음인 'ㅇ'을 형상화한 '₩' 기호를 사용합니다.\"},\n",
    "    {\"instruction\": \"금(Gold)의 원소 기호는 무엇인가요?\", \"output\": \"금의 원소 기호는 Au입니다.\"},\n",
    "    {\"instruction\": \"남극과 북극 중 더 추운 곳은 어디인가요?\", \"output\": \"대륙 위에 얼음이 쌓인 남극이 바다 위에 얼음이 떠 있는 북극보다 훨씬 더 춥습니다.\"},\n",
    "    {\"instruction\": \"경주 불국사에 있는 두 개의 유명한 탑 이름은?\", \"output\": \"다보탑과 석가탑(삼층석탑)입니다.\"},\n",
    "    {\"instruction\": \"꿀벌이 꽃 사이를 다니며 하는 중요한 역할은?\", \"output\": \"꽃가루를 옮겨 식물의 수정을 돕는 '수분' 역할을 합니다.\"},\n",
    "\n",
    "    # --- 실용적인 가이드 및 조언 ---\n",
    "    {\"instruction\": \"컴퓨터 속도를 빠르게 하는 법 3가지.\", \"output\": \"1. 불필요한 프로그램 삭제 2. 시작 프로그램 정리 3. SSD 여유 공간 확보.\"},\n",
    "    {\"instruction\": \"잠이 잘 오는 방법을 알려줘.\", \"output\": \"규칙적인 수면 시간을 유지하고, 취침 전 카페인 섭취를 피하며 따뜻한 우유를 마시는 것이 도움이 됩니다.\"},\n",
    "    {\"instruction\": \"여권 발급 시 필요한 준비물은?\", \"output\": \"신분증, 최근 6개월 이내 촬영한 여권용 사진 1매, 수수료가 필요합니다.\"},\n",
    "    {\"instruction\": \"운동 전 스트레칭의 장점은?\", \"output\": \"근육의 긴장을 풀고 유연성을 높여 부상을 예방해 줍니다.\"},\n",
    "    {\"instruction\": \"효과적인 공부법 하나만 알려줘.\", \"output\": \"학습한 내용을 남에게 가르치듯이 말해보는 '인출 연습'이 매우 효과적입니다.\"},\n",
    "    {\"instruction\": \"면접에서 좋은 인상을 남기는 법.\", \"output\": \"밝은 미소로 인사하고, 질문의 의도를 정확히 파악하여 결론부터 대답하는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"겨울철 실내 습도 조절 방법.\", \"output\": \"가습기를 사용하거나 젖은 수건을 걸어두고, 실내 식물을 키우는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"맛있는 라면 끓이는 비법은?\", \"output\": \"물이 끓기 전에 스프를 먼저 넣고, 면을 들었다 놨다 하며 공기와 접촉시키면 면발이 더 쫄깃해집니다.\"},\n",
    "    {\"instruction\": \"커피 얼룩 지우는 방법.\", \"output\": \"주방세제와 식초를 섞어 얼룩 부위를 살살 문지른 후 미온수로 헹궈내세요.\"},\n",
    "    {\"instruction\": \"화분 물 주는 시기 확인법.\", \"output\": \"손가락으로 겉흙을 2cm 정도 찔러보았을 때 속까지 말라 있으면 물을 줄 때입니다.\"},\n",
    "    {\"instruction\": \"눈 피로를 줄이는 방법.\", \"output\": \"20분마다 20피트(약 6미터) 밖을 20초 동안 바라보는 20-20-20 법칙을 실천하세요.\"},\n",
    "    {\"instruction\": \"효과적인 분리수거 방법은?\", \"output\": \"비우고, 헹구고, 분리하고, 섞지 않는 4가지 원칙을 지켜야 합니다.\"},\n",
    "    {\"instruction\": \"옷에 묻은 볼펜 자국 지우는 법.\", \"output\": \"물파스나 알코올을 묻혀 살살 문지른 뒤 세탁하면 깨끗해집니다.\"},\n",
    "    {\"instruction\": \"스마트폰 배터리 수명을 늘리는 법.\", \"output\": \"화면 밝기를 조절하고, 배터리를 0%까지 방전시키지 않는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"발표할 때 긴장을 줄이는 팁.\", \"output\": \"심호흡을 크게 하고, 청중 중 한 명과 눈을 맞추며 대화하듯 말해보세요.\"},\n",
    "    {\"instruction\": \"냉장고 냄새 제거하는 법.\", \"output\": \"먹다 남은 소주나 커피 찌꺼기, 베이킹소다를 넣어두면 효과적입니다.\"},\n",
    "    {\"instruction\": \"신발 냄새를 없애는 간단한 방법.\", \"output\": \"말린 녹차 티백이나 신문지를 신발 속에 넣어두면 습기와 냄새를 잡아줍니다.\"},\n",
    "    {\"instruction\": \"딸기 보관법을 알려줘.\", \"output\": \"씻지 않은 상태로 키친타월에 싸서 냉장 보관하는 것이 가장 오래 갑니다.\"},\n",
    "    {\"instruction\": \"좋은 베개를 고르는 기준.\", \"output\": \"목의 C자 곡선을 유지해주고 적당한 탄성이 있는 베개를 선택하세요.\"},\n",
    "    {\"instruction\": \"전자레인지 청소 꿀팁.\", \"output\": \"물에 레몬이나 식초를 넣어 5분간 돌린 후 수증기로 불어난 때를 닦아내세요.\"},\n",
    "    {\"instruction\": \"전자레인지에 절대 넣으면 안 되는 물건은?\", \"output\": \"금속 용기, 알루미늄 호일, 껍질을 벗기지 않은 달걀 등은 폭발 위험이 있어 넣으면 안 됩니다.\"},\n",
    "    {\"instruction\": \"여름철 냉방병 예방 방법.\", \"output\": \"실내외 온도 차를 5도 이내로 유지하고, 주기적으로 환기를 시키는 것이 좋습니다.\"},\n",
    "    {\"instruction\": \"유통기한과 소비기한의 차이는?\", \"output\": \"유통기한은 제품을 판매할 수 있는 기간이고, 소비기한은 먹어도 안전에 이상이 없는 기간입니다.\"},\n",
    "    {\"instruction\": \"목감기에 좋은 차를 추천해줘.\", \"output\": \"따뜻한 생강차, 도라지차, 배숙 등이 기관지 보호에 효과적입니다.\"},\n",
    "    {\"instruction\": \"에어컨 전기세를 아끼는 방법.\", \"output\": \"처음 켤 때 강풍으로 설정하여 온도를 빠르게 낮춘 뒤 적정 온도를 유지하는 것이 효율적입니다.\"},\n",
    "    {\"instruction\": \"손가락에 박힌 가시를 빼는 팁.\", \"output\": \"가시가 박힌 부위를 따뜻한 물에 불린 뒤 집게를 소독하여 결 방향대로 뽑으세요.\"},\n",
    "    {\"instruction\": \"우유가 상했는지 확인하는 법.\", \"output\": \"찬물에 우유를 한 방울 떨어뜨렸을 때, 퍼지지 않고 가라앉으면 신선한 상태입니다.\"},\n",
    "    {\"instruction\": \"청소기 흡입력이 약해졌을 때 체크할 곳.\", \"output\": \"먼지 통이 가득 찼는지, 필터에 먼지가 끼었는지, 흡입구에 이물질이 있는지 확인하세요.\"},\n",
    "    {\"instruction\": \"숙취 해소에 좋은 음식.\", \"output\": \"아스파라긴산이 풍부한 콩나물국이나 수분 보충을 돕는 꿀물이 좋습니다.\"},\n",
    "    {\"instruction\": \"올바른 칫솔질 방법인 '회전법'이란?\", \"output\": \"손목을 돌려 잇몸에서 치아 방향으로 쓸어내리듯 닦는 방식입니다.\"},\n",
    "\n",
    "    # --- 언어 작업: 요약, 번역, 문장 변환 ---\n",
    "    {\"instruction\": \"'밥 먹었니?'를 존댓말로 바꿔줘.\", \"output\": \"식사하셨어요?\"},\n",
    "    {\"instruction\": \"'I love you'를 한국어로 번역해.\", \"output\": \"사랑해 또는 사랑합니다.\"},\n",
    "    {\"instruction\": \"다음 문장을 요약해: 인공지능 기술이 발전함에 따라 우리의 일상은 편리해졌지만 사생활 침해 우려도 커지고 있다.\", \"output\": \"AI 발전으로 일상은 편리해졌으나 사생활 침해 문제가 제기되고 있다.\"},\n",
    "    {\"instruction\": \"'슬프다'의 반대말은?\", \"output\": \"기쁘다입니다.\"},\n",
    "    {\"instruction\": \"'노력은 배신하지 않는다'는 속담의 뜻은?\", \"output\": \"무엇이든 열심히 하면 반드시 좋은 결과를 얻게 된다는 뜻입니다.\"},\n",
    "    {\"instruction\": \"다음 단어들을 사용해 한 문장을 만들어줘: 강아지, 공원, 산책.\", \"output\": \"강아지와 함께 공원에서 즐겁게 산책을 했습니다.\"},\n",
    "    {\"instruction\": \"감동적인 이별 노래 제목 3개 추천해줘.\", \"output\": \"1. 안녕 (폴킴) 2. 보고 싶다 (김범수) 3. 서른 즈음에 (김광석).\"},\n",
    "    {\"instruction\": \"'고맙습니다'를 영어로 번역하면?\", \"output\": \"Thank you입니다.\"},\n",
    "    {\"instruction\": \"'사과'라는 단어로 짧은 2행시를 지어줘.\", \"output\": \"사: 사시사철 먹어도 맛있는 과일 / 과: 과연 사과가 최고야.\"},\n",
    "    {\"instruction\": \"어색한 문장을 수정해줘: 나는 학교에 갔고 밥을 먹었다 하지만 졸렸다.\", \"output\": \"나는 학교에 가서 밥을 먹었지만 졸음이 쏟아졌다.\"},\n",
    "    {\"instruction\": \"'밥 먹었어?'를 영어로.\", \"output\": \"Did you eat?\"},\n",
    "    {\"instruction\": \"'Happy New Year'를 한국어로.\", \"output\": \"새해 복 많이 받으세요.\"},\n",
    "    {\"instruction\": \"'열심히 공부하다'를 네 글자 한자성어로?\", \"output\": \"형설지공(螢雪之功)입니다.\"},\n",
    "    {\"instruction\": \"'희망'의 반대말은?\", \"output\": \"절망입니다.\"},\n",
    "    {\"instruction\": \"'천 리 길도 한 걸음부터'라는 속담의 의미는?\", \"output\": \"무슨 일이든 시작이 중요하다는 뜻입니다.\"},\n",
    "    {\"instruction\": \"다음 문장의 맞춤법을 고쳐줘: 어의가 없네.\", \"output\": \"어이가 없네로 고쳐야 합니다.\"},\n",
    "    {\"instruction\": \"단어 '바다'를 활용해 짧은 시를 써줘.\", \"output\": \"끝없이 푸른 바다, 파도 소리에 내 마음도 춤을 춘다.\"},\n",
    "    {\"instruction\": \"'책'이라는 단어로 1행시를 지어줘.\", \"output\": \"책: 책 속에 세상의 모든 지혜가 담겨 있다.\"},\n",
    "    {\"instruction\": \"다음 문장을 높임말로: 나는 어제 도서관에 갔다.\", \"output\": \"저는 어제 도서관에 갔습니다.\"},\n",
    "    {\"instruction\": \"핵심어 '여름, 수박, 시원함'으로 문장을 만들어줘.\", \"output\": \"여름에는 시원한 수박을 먹는 것이 최고입니다.\"},\n",
    "    {\"instruction\": \"'식사 맛있게 하세요'를 친구에게 말하듯 반말로 바꿔줘.\", \"output\": \"맛있게 먹어!\"},\n",
    "    {\"instruction\": \"'Thank you for your help'를 한국어로 자연스럽게 번역해.\", \"output\": \"도와주셔서 감사합니다.\"},\n",
    "    {\"instruction\": \"다음 문장의 어색한 부분을 고쳐줘: 저는 밥을 먹었다.\", \"output\": \"저는 밥을 먹었습니다. (또는 '나는 밥을 먹었다'로 주어와 어미를 맞춥니다.)\"},\n",
    "    {\"instruction\": \"'시작이 반이다'라는 격언을 영어로?\", \"output\": \"Well begun is half done.\"},\n",
    "    {\"instruction\": \"단어 '구름'으로 감성적인 문장을 만들어줘.\", \"output\": \"하늘 위 솜사탕 같은 구름이 내 마음을 포근하게 감싸 안아주는 오후입니다.\"},\n",
    "    {\"instruction\": \"'노트북'으로 3행시 지어줘.\", \"output\": \"노: 노력하는 당신의 모습 / 트: 트여 있는 미래를 위해 / 북: 북돋워 줄 최고의 도구.\"},\n",
    "    {\"instruction\": \"'책상'과 '연필'을 넣어 한 문장을 써줘.\", \"output\": \"책상 위에 놓인 연필로 나의 소중한 꿈을 적어 내려갑니다.\"},\n",
    "    {\"instruction\": \"매우 기쁜 상태를 나타내는 사자성어는?\", \"output\": \"입이 귀에 걸릴 정도로 즐겁다는 뜻의 '환호작약(歡呼雀躍)' 등이 있습니다.\"},\n",
    "    {\"instruction\": \"다음 문장을 요약해: 운동은 신체 건강뿐만 아니라 엔도르핀 생성을 돕기에 정신 건강에도 매우 긍정적인 영향을 미친다.\", \"output\": \"운동은 신체와 정신 건강 모두에 유익하다.\"},\n",
    "    {\"instruction\": \"'반갑다'의 높임말 표현은?\", \"output\": \"반갑습니다 또는 뵙게 되어 영광입니다.\"},\n",
    "\n",
    "    # --- 창의적 글쓰기 및 상황극 ---\n",
    "    {\"instruction\": \"비 오는 날 어울리는 점심 메뉴 제안해줘.\", \"output\": \"비 오는 날에는 따뜻한 국물이 있는 칼국수나 노릇하게 구운 파전을 추천합니다.\"},\n",
    "    {\"instruction\": \"새해 인사말 문구 작성해줘.\", \"output\": \"새해 복 많이 받으시고, 올 한 해 소망하시는 모든 일이 이루어지길 기원합니다.\"},\n",
    "    {\"instruction\": \"생일 축하 편지 첫 줄을 써줘.\", \"output\": \"세상에서 가장 특별한 너의 생일을 진심으로 축하해!\"},\n",
    "    {\"instruction\": \"여행 가방 쌀 때 잊지 말아야 할 것.\", \"output\": \"휴대폰 충전기와 보조배터리, 그리고 비상약은 꼭 챙기세요.\"},\n",
    "    {\"instruction\": \"고민 상담: 공부가 너무 하기 싫을 때 어떡해?\", \"output\": \"딱 10분만 앉아있자고 자신과 약속해보세요. 시작이 가장 어렵답니다.\"},\n",
    "    {\"instruction\": \"오늘의 행운 메시지를 적어줘.\", \"output\": \"당신이 미소 지을 때 행운은 당신 곁으로 다가옵니다. 오늘 하루도 웃음 가득하세요!\"},\n",
    "    {\"instruction\": \"친구에게 빌린 책을 돌려줄 때 쓰는 메모.\", \"output\": \"덕분에 정말 재미있게 읽었어! 빌려줘서 고마워.\"},\n",
    "    {\"instruction\": \"이사 가는 친구에게 응원 메시지.\", \"output\": \"새로운 집에서 기분 좋은 일들만 가득하길 바랄게! 이사 축하해.\"},\n",
    "    {\"instruction\": \"카페 사장님이 손님에게 할 수 있는 인사.\", \"output\": \"어서 오세요! 따뜻하고 향긋한 커피 한 잔과 함께 편안한 시간 보내세요.\"},\n",
    "    {\"instruction\": \"우울할 때 들으면 좋은 말.\", \"output\": \"지금 충분히 잘하고 있어. 조금 쉬어가도 괜찮아.\"},\n",
    "    {\"instruction\": \"월요일 아침 출근하는 직장인의 마음을 한 문장으로.\", \"output\": \"커피 한 잔의 힘을 빌려 무거운 몸을 이끌고 다시 일상을 시작합니다.\"},\n",
    "    {\"instruction\": \"친구에게 보내는 응원 메시지.\", \"output\": \"네가 지금까지 해온 노력을 믿어. 넌 충분히 해낼 수 있어!\"},\n",
    "    {\"instruction\": \"가상의 마법 지팡이 판매 문구.\", \"output\": \"당신의 상상을 현실로 만드는 단 하나의 선택, 미라클 지팡이를 만나보세요.\"},\n",
    "    {\"instruction\": \"반려견이 주인에게 하고 싶은 말 (가상).\", \"output\": \"주인님, 오늘도 퇴근하고 나랑 신나게 놀아줄 거지? 기다리고 있을게!\"},\n",
    "    {\"instruction\": \"가을 산책길에 어울리는 음악 장르는?\", \"output\": \"잔잔한 어쿠스틱 기타 선율이나 재즈 음악을 추천합니다.\"},\n",
    "    {\"instruction\": \"첫 데이트 장소를 추천해줘.\", \"output\": \"대화하기 좋은 분위기 있는 카페나 산책로가 있는 공원을 추천합니다.\"},\n",
    "    {\"instruction\": \"공부 자극이 되는 한마디.\", \"output\": \"오늘의 노력이 내일의 당신을 만듭니다.\"},\n",
    "    {\"instruction\": \"실패해서 좌절한 사람에게 해줄 말.\", \"output\": \"실패는 더 큰 성공을 위한 배움의 과정일 뿐이에요. 다시 일어설 수 있어요.\"},\n",
    "    {\"instruction\": \"나만의 스트레스 해소법 추천.\", \"output\": \"좋아하는 음악을 크게 틀어놓고 청소를 하거나 일기를 써보세요.\"},\n",
    "    {\"instruction\": \"졸업하는 학생들에게 전하는 축사 첫 문장.\", \"output\": \"새로운 시작을 앞둔 여러분의 앞날에 빛나는 영광이 가득하기를 바랍니다.\"},\n",
    "    {\"instruction\": \"혼자 여행하기 좋은 국내 장소 추천해줘.\", \"output\": \"대중교통이 편리하고 볼거리가 많은 경주나 강릉을 추천합니다.\"},\n",
    "    {\"instruction\": \"월요병을 극복하는 나만의 방법 제안.\", \"output\": \"일요일 밤에 미리 좋아하는 노래 리스트를 만들어두고 월요일 아침에 들어보세요.\"},\n",
    "    {\"instruction\": \"부모님 결혼기념일 축하 문구 작성.\", \"output\": \"사랑하는 엄마 아빠, 두 분의 소중한 날을 진심으로 축하드려요. 항상 건강하고 행복하세요!\"},\n",
    "    {\"instruction\": \"자신감이 떨어졌을 때 읽으면 좋은 글귀.\", \"output\": \"당신은 당신이 생각하는 것보다 훨씬 더 강하고 아름다운 사람입니다.\"},\n",
    "    {\"instruction\": \"반려묘가 집사에게 바라는 점 (가상).\", \"output\": \"가끔은 조용히 내 옆에 앉아서 쓰다듬어 주는 것만으로도 충분히 행복해옹!\"},\n",
    "    {\"instruction\": \"집들이 선물로 센스 있는 아이템 3개.\", \"output\": \"1. 세련된 디자인의 디퓨저 2. 실용적인 수건 세트 3. 예쁜 실내 공기정화 식물.\"},\n",
    "    {\"instruction\": \"다이어트 결심이 흔들릴 때 해줄 말.\", \"output\": \"오늘 하루 실패했다고 끝이 아니에요. 내일 다시 시작하면 됩니다. 포기하지 마세요!\"},\n",
    "    {\"instruction\": \"퇴사하는 동료에게 전하는 작별 인사.\", \"output\": \"그동안 함께 일해서 정말 즐거웠습니다. 새로운 시작을 진심으로 응원할게요!\"},\n",
    "    {\"instruction\": \"주말에 집에서 즐길 수 있는 취미는?\", \"output\": \"컬러링북 색칠하기, 간단한 홈베이킹, 또는 미뤄둔 영화 정주행을 추천합니다.\"},\n",
    "    {\"instruction\": \"화가 났을 때 마음을 가라앉히는 법.\", \"output\": \"숫자를 1부터 10까지 천천히 세며 깊게 심호흡을 해보세요.\"},\n",
    "\n",
    "    # --- 논리 및 수학/코딩 (10개) ---\n",
    "    {\"instruction\": \"15 + 27의 결과는?\", \"output\": \"42입니다.\"},\n",
    "    {\"instruction\": \"사각형의 넓이를 구하는 공식은?\", \"output\": \"가로 길이 x 세로 길이입니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 'Hello World'를 출력하는 코드는?\", \"output\": \"print('Hello World')\"},\n",
    "    {\"instruction\": \"100의 10%는 얼마인가요?\", \"output\": \"10입니다.\"},\n",
    "    {\"instruction\": \"반지름이 5인 원의 넓이는? (원주율은 3.14)\", \"output\": \"넓이는 78.5입니다. (5 * 5 * 3.14)\"},\n",
    "    {\"instruction\": \"3의 3제곱은?\", \"output\": \"27입니다.\"},\n",
    "    {\"instruction\": \"짝수란 무엇인가요?\", \"output\": \"2로 나누어떨어지는 정수를 말합니다.\"},\n",
    "    {\"instruction\": \"자바스크립트에서 현재 시간을 가져오는 코드는?\", \"output\": \"new Date()\"},\n",
    "    {\"instruction\": \"가장 작은 소수는 무엇인가요?\", \"output\": \"가장 작은 소수는 2입니다.\"},\n",
    "    {\"instruction\": \"HTML에서 하이퍼링크를 만드는 태그는?\", \"output\": \"<a> 태그를 사용합니다.\"},\n",
    "    {\"instruction\": \"8 * 9의 결과는?\", \"output\": \"72입니다.\"},\n",
    "    {\"instruction\": \"1부터 10까지의 합은?\", \"output\": \"55입니다.\"},\n",
    "    {\"instruction\": \"삼각형의 내각의 총합은?\", \"output\": \"180도입니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 리스트에 요소를 추가하는 함수는?\", \"output\": \"append() 함수를 사용합니다.\"},\n",
    "    {\"instruction\": \"HTML에서 이미지를 넣는 태그는?\", \"output\": \"<img src='이미지경로'> 태그를 사용합니다.\"},\n",
    "    {\"instruction\": \"7은 소수인가요?\", \"output\": \"네, 7은 1과 자기 자신만으로 나누어떨어지는 소수입니다.\"},\n",
    "    {\"instruction\": \"2의 10제곱은 얼마인가요?\", \"output\": \"1024입니다.\"},\n",
    "    {\"instruction\": \"비트(bit)란 무엇인가요?\", \"output\": \"컴퓨터에서 정보를 나타내는 최소 단위로 0 또는 1을 가집니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 변수의 타입을 확인하는 함수는?\", \"output\": \"type() 함수입니다.\"},\n",
    "    {\"instruction\": \"직사각형의 둘레를 구하는 공식은?\", \"output\": \"2 * (가로 길이 + 세로 길이)입니다.\"},\n",
    "    {\"instruction\": \"56 나누기 8은?\", \"output\": \"7입니다.\"},\n",
    "    {\"instruction\": \"원주율(π)의 대략적인 수치는?\", \"output\": \"약 3.14입니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 'Hello'와 'World' 두 문자열을 합치는 코드는?\", \"output\": \"'Hello' + 'World'\"},\n",
    "    {\"instruction\": \"1분은 몇 초인가요?\", \"output\": \"60초입니다.\"},\n",
    "    {\"instruction\": \"직각삼각형에서 가장 긴 변의 이름은?\", \"output\": \"빗변입니다.\"},\n",
    "    {\"instruction\": \"컴퓨터의 뇌라고 불리는 핵심 부품은?\", \"output\": \"CPU(중앙처리장치)입니다.\"},\n",
    "    {\"instruction\": \"파이썬에서 리스트의 마지막 요소를 꺼내는 함수는?\", \"output\": \"pop() 함수입니다.\"},\n",
    "    {\"instruction\": \"이진수 1010을 십진수로 바꾸면?\", \"output\": \"10입니다.\"},\n",
    "    {\"instruction\": \"HTML에서 글자 크기를 가장 크게 만드는 제목 태그는?\", \"output\": \"<h1> 태그입니다.\"},\n",
    "    {\"instruction\": \"0으로 숫자를 나눌 수 있나요?\", \"output\": \"수학적으로 정의되지 않으므로 나눌 수 없습니다.\"},\n",
    "]\n",
    "\n",
    "# 전체 데이터 100여개를 루프를 통해 json으로 저장\n",
    "with open(\"./input/tinyllama_train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data: # 100개 데이터라고 가정\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"총 {len(data)}개의 데이터가 tinyllama_train_data.jsonl로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a8588d-3a54-4616-ada8-092b34d1322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankus/miniconda3/envs/test_vLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 152 examples [00:00, 55855.46 examples/s]\n",
      "Map: 100%|██████████| 152/152 [00:00<00:00, 3809.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료된 데이터 샘플: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 1. 데이터셋 로드\n",
    "dataset = Dataset.from_json(\"./input/tinyllama_train_data.jsonl\")\n",
    "\n",
    "# 2. 모델 및 토크나이저 로드\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.padding_side = \"right\" # 학습 시에는 오른쪽 패딩 권장\n",
    "\n",
    "# 3. 데이터 전처리 함수 수정\n",
    "def tokenize_function(examples):\n",
    "    # TinyLlama가 이해하기 쉬운 프롬프트 형태로\n",
    "    # 포맷: ### 질문: ... \\n### 답변: ... <|endoftext|>\n",
    "    texts = [\n",
    "        f\"### 질문: {inst}\\n### 답변: {out}{tokenizer.eos_token}\"\n",
    "        for inst, out in zip(examples[\"instruction\"], examples[\"output\"])\n",
    "    ]\n",
    "    \n",
    "    # 토큰화 수행\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# 전처리 적용 (기존 instruction, output 컬럼은 제거)\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"instruction\", \"output\"]\n",
    ")\n",
    "\n",
    "print(f\"전처리 완료된 데이터 샘플: {tokenized_datasets[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:30:45.077866Z",
     "start_time": "2025-12-31T05:30:36.262867Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# # 1. 예시 데이터셋 생성 TODO .txt나 .json 파일을 로드하는 방식으로 수정 필요\n",
    "# data = [\n",
    "#     {\"text\": \"질문: 사과의 색깔은 무엇인가요? 답변: 사과는 보통 빨간색이나 초록색입니다.\"},\n",
    "#     {\"text\": \"질문: 하늘은 왜 파란가요? 답변: 빛의 산란 현상 때문입니다.\"},\n",
    "# ]\n",
    "# # # 텍스트 데이터를 토큰화 by Dataset\n",
    "# dataset = Dataset.from_list(data)\n",
    "\n",
    "# # 2. 모델 및 토크나이저 로드 (작은 모델: TinyLlama)\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token # 패딩 토큰 설정\n",
    "\n",
    "# # 3. 데이터 전처리 함수\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12529b072f07b4",
   "metadata": {},
   "source": [
    "# 3. 좆밥 GPU 위한 효율적인 학습 기법 (LoRA)\n",
    "#### - GPU 메모리(예: 11GB 내외의 RTX 2080 Ti)가 부족하여, 모델 전체를 학습시키는 대신 일부 파라미터만 학습시키는 LoRA(Low-Rank Adaptation) 방식 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80773b71e7bb59a2",
   "metadata": {},
   "source": [
    "#### 1) (가장 중요) 가중치 정밀도 및 로드 설정 (Quantization)\n",
    "###### 모델 자체의 무게를 줄여 GPU의 기본 점유 공간을 확보하는 것이 가장 중요\n",
    "##### - load_in_4bit=True (QLoRA): 모델을 4비트로 양자화하여 로드. 16비트 대비 메모리 사용량 약 1/4로 줄임. 20280ti 11GB 메모리라면 7B 모델도 간신히 올릴 수 있을까?\n",
    "###### - bnb_4bit_compute_dtype=torch.bfloat16: 연산 시의 정밀도 설정. float16보다 bfloat16이 학습 안정성이 높지만, 구형 GPU(2080 Ti 등)는 float16을 사용해야 그나마 속도 나옴.\n",
    "###### - bnb_4bit_quant_type=\"nf4\": 일반적인 4비트보다 더 정밀한 NormalFloat4 방식을 사용하여 양자화로 인한 성능 저하를 최소화\n",
    "\n",
    "#### 2) LoRA 하이퍼파라미터 (PEFT Config)\n",
    "###### 학습 대상이 되는 파라미터의 수 결정.\n",
    "###### - r (Rank): LoRA 행렬의 크기. 값이 클수록 표현력은 좋아지지만 메모리 사용량 커짐. 보통 8 또는 16을 권장, 메모리가 극도로 부족하면 4까지 도전!!!\n",
    "###### - target_modules: 학습할 레이어를 지정. [\"q_proj\", \"v_proj\"]만 지정하면 메모리를 아낄 수 있고, [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]처럼 모두 지정하면 성능은 좋아지지만 메모리 부하 큼.\n",
    "###### - task_type=\"CAUSAL_LM\": 텍스트 생성 모델의 경우 이 설정을 명시하여 불필요한 레이어가 학습되는 것을 방지.\n",
    "\n",
    "#### 3) 배치 사이즈 및 그래디언트 설정 (Training Arguments)\n",
    "###### 학습 데이터가 GPU를 거칠 때 발생하는 부하를 조절.\n",
    "###### - per_device_train_batch_size=1: 실제 GPU에 한 번에 올리는 데이터 개수. !!! OOM이 발생하면 무조건 1로 설정 !!!\n",
    "###### - gradient_accumulation_steps: 부족한 배치 사이즈를 보완하는 핵심 변수. 예를 들어 배치 1로 설정하고 이 값을 16으로 주면, 16번의 계산 결과를 모아서 한 번 가중치를 업데이트. 실제 배치 사이즈 16의 효과를 내면서 메모리는 1만큼만 사용(혜자닷!).\n",
    "##### - gradient_checkpointing=True: 역전파(Backpropagation) 과정에서 필요한 중간 계산값을 모두 저장하지 않고, 필요할 때 다시 계산. 연산 속도는 약 30% 느려지지만 메모리 점유율을 획기적으로 낮춤(약간 대출 느낌?).\n",
    "\n",
    "#### 4) 시퀀스 길이 조절 (Data Processing)\n",
    "###### 데이터 한 건의 길이가 길수록 메모리 사용량은 기하급수적으로 늘어남.\n",
    "##### - max_seq_length (또는 max_length): 모델이 한 번에 처리하는 토큰 수. 기본 2048, 4096, 메모리가 부족하면 512나 1024로.\n",
    "###### - group_by_length=True: 길이가 비슷한 데이터끼리 묶어서 배치 학습을 진행. 패딩(Padding) 토큰 낭비를 줄여 효율을 높임.\n",
    "\n",
    "#### 5) 최적화 도구 (Optimizer & Offloading)\n",
    "###### 학습 시 사용되는 수학적 최적화 도구의 메모리 점유를 줄임.\n",
    "##### - optim=\"paged_adamw_8bit\" 또는 \"paged_adamw_32bit\": 최적화 도구의 상태값을 8비트로 압축하거나, 필요 없는 데이터를 CPU로 잠시 넘기는(Paged) 기능을 활성화. 일반 AdamW보다 메모리를 훨씬 적게 사용.\n",
    "###### - fp16=True: 16비트 혼합 정밀도 학습을 사용하여 32비트 대비 메모리 사용량을 절반으로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2783c7076b2ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:42:03.453416Z",
     "start_time": "2025-12-31T05:30:45.094698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    # 100개 데이터라면 표현력을 높이기 위해 8보다 16을 추천 (메모리 허용 시)\n",
    "    r=16,  # # r(Rank): LoRA 행렬의 크기. 값이 클수록 표현력은 좋아지지만 메모리 사용량 커짐. 보통 8 또는 16을 권장, 메모리가 극도로 부족하면 4까지 도전!!!\n",
    "    lora_alpha=32,\n",
    "    # [수정] q, v 뿐만 아니라 모든 리니어 레이어를 타겟으로 잡아 학습 효율 극대화.\n",
    "    # 소형 모델일수록 더 많은 레이어를 학습하는 것이 한국어 습득에 유리.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    lora_dropout=0.1,    # 데이터가 적으므로 드롭아웃을 살짝 높여 과적합을 방지.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters() # 학습 가능한 파라미터 비중 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5286ea8a54898",
   "metadata": {},
   "source": [
    "# 4. 학습 실행 (Trainer API)\n",
    "### Hugging Face의 Trainer를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa322093e6a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/380 07:16 < 00:04, 0.86 it/s, Epoch 9.87/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "\n",
    "    # 1. 학습 루프 설정\n",
    "    num_train_epochs=10,             # 전체 데이터 반복 횟수  # # 100개 데이터 기준 5~10회 추천\n",
    "    learning_rate=2e-4,             # 학습률\n",
    "    lr_scheduler_type=\"cosine\",       # 학습률을 서서히 줄여 안정적으로 수렴\n",
    "    warmup_ratio=0.1,                 # 초기 10% 구간은 학습률을 서서히 올림\n",
    "    \n",
    "    # 2. 배치 사이즈 및 메모리 최적화 (2080 Ti 11GB 기준)\n",
    "    per_device_train_batch_size=1,   # 메모리 상황에 따라 조절  # GPU 한 장당 배치 1 (OOM 방지)  1 * 4 = 16\n",
    "    gradient_accumulation_steps=4,  # 실제 배치 사이즈 = 4 * 4 = 16 (per_device_train_batch_size=4 인 경우)\n",
    "    gradient_checkpointing=True,      # 메모리 사용량 획기적 절감 (필수)\n",
    "\n",
    "    # 3. 정밀도 및 하드웨어 가속\n",
    "    fp16=True,                      # 16비트 혼합 정밀도 학습 (속도 향상)\n",
    "    # [수정] 2080 Ti라면 8bit 옵티마이저가 메모리 관리에 더 유리\n",
    "    optim=\"paged_adamw_8bit\",       # 메모리 효율적인 옵티마이저  paged_adamw_8bit, paged_adamw_32bit\n",
    "\n",
    "    # 4. 로그 및 저장 전략\n",
    "    logging_steps=5,  # 데이터가 적으므로 더 자주 로그 확인\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,               # 디스크 용량 관리를 위해 최신 체크포인트만 유지\n",
    "\n",
    "    # 5. 성능 최적화\n",
    "    max_grad_norm=1.0,                # 0.3은 너무 엄격할 수 있어 1.0으로 완화하여 학습 속도 개선\n",
    "    weight_decay=0.05,                # 데이터가 적을 때 과적합 방지를 위해 조금 더 강화\n",
    "    group_by_length=True,             # 속도 향상을 위해 길이가 비슷한 데이터끼리 묶음, Padding 많이 들어간 데이터는 학습 효율 낮음. 길이가 비슷한 데이터끼리 배치로 묶어 학습 효율을 약 10~20% 높임.\n",
    ")\n",
    "\n",
    "# 4-1. 그래디언트 체크포인팅 사용 시 필수 설정\n",
    "if getattr(training_args, \"gradient_checkpointing\", False):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    # 아래 설정이 누락되면 해당 에러가 발생 가능\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "# 4-2. LoRA 파라미터가 제대로 잡혔는지 확인 (중요)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 5. 트레이너 초기화 및 학습 시작\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 6. 학습된 모델 저장\n",
    "model.save_pretrained(\"./output/my_llm\")\n",
    "tokenizer.save_pretrained(\"./output/my_llm\") # 모델과 같은 경로에 저장 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86ee984ddf1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_vLLM",
   "language": "python",
   "name": "test_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
